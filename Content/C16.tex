\newpage

\section[Day 16: Multivariable Functions]{ Multivariable Functions }

\subsection{ Linear Transformations }

    \begin{definition}{Vector Spaces}{14cm}
        \begin{enumerate}[label=(\alph*), leftmargin=0.5cm, itemsep=0.1cm]
            \item {\color{lblue} Vector Space}
            
                A nonempty set X $\subset$ $\mathbb{R}^n$ is a vector space if
                for all x,y $\in$ X and scalar c:
        
                \hspace{0.5cm}
                x+y $\in$ X
                \hspace{1cm}
                cx $\in$ X

                Null vector 0 is also defined as 0 = (0,...,0) $\in$ $\mathbb{R}^k$.
            
            \item {\color{lblue} Linear Combinations and Span}
            
                For scalars $c_1,...,c_k$, a linear combination of
                $x_1,...,x_k$ $\in$ $\mathbb{R}^n$:
        
                \hspace{0.5cm}
                $c_1x_1 + ... + c_kx_k$
        
                The span of $x_1,...,x_k$ is the set of all linear combinations
                of $x_1,...,x_k$.

            \item {\color{lblue} Independence and Dimension}
                
                If $c_1x_1 + ... + c_kx_k$ = 0 only if $c_1$ = ... = $c_k$ = 0,
                then $x_1,...,x_k$ are independent.
                Any independent set does not contain 0 since
                $c0 + c_1x_1 + ... + c_kx_k$ = 0 holds true for
                $c,0,...,0$ where c is any number, not just $0,0,...,0$.

                \vspace{0.2cm}

                If vector space X have r independent vectors, but not r+1
                independent vectors, then dim(X) = r.
                The set \{0\} has dimension 0.

            \item {\color{lblue} Basis}
                
                If $x_1,..,x_k$ $\in$ X are independent and spans X, then
                $x_1,..,x_k$ is a basis of X.

                Thus, for every x $\in$ X:

                \hspace{0.5cm}
                Since $x_1,..,x_k$ spans X, there exists $c_1,...,c_k$ such that
                x = $c_1x_1 + ... + c_kx_k$.

                \hspace{0.5cm}
                Since $x_1,..,x_k$ are independent, then such $c_1,...,c_k$
                are unique else there
                
                \hspace{0.5cm}
                are $a_1,...,a_k$ where at least one
                $a_i$ $\not =$ $c_i$ such that:

                \hspace{1cm}
                x = $a_1x_1 + ... + a_kx_k$
                \hspace{0.5cm}
                $\Rightarrow$
                \hspace{0.5cm}
                0 = x - x = $(a_1-c_1)x_1 + ... + (a_k-c_k)x_k$

                \hspace{0.5cm}
                where at least one $(a_i - c_i)$ $\not =$ 0
                contradicting $x_1,...,x_k$ are independent.

                The $c_1,...,c_k$ are called the coordinates of x
                with respect to basis $x_1,...,x_k$.

            \item Standard Basis of $\mathbb{R}^k$
            
                Let $e_i$
                = $(\underset{\scriptscriptstyle 1}{0},...,
                    \underset{\scriptscriptstyle i-1}{0},
                    \underset{\scriptscriptstyle i}{1},
                    \underset{\scriptscriptstyle i+1}{0},...,
                    \underset{\scriptscriptstyle k}{0})$
                $\in$ $\mathbb{R}^k$.

                Thus, $e_1,...,e_k$ is a basis for $\mathbb{R}^k$
                where any x = ($x_1,....,x_k$) = $x_1e_1 + .... + x_ke_k$.
        \end{enumerate}
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{dim(X) $\leq$ (\# vectors that span X)}{14cm}
        If vector space X is spanned by r vectors, then dim(X) $\leq$ r.
    \end{wtheorem}

    \begin{proof}
        If dim(X) $>$ r, then there are at minimum r+1 independent vectors
        that spans X which contradicts that X is spanned by r vectors.

        Let X be spanned by $x_1,...,x_r$ $\not =$ 0.
        If $x_1,...,x_r$ are independent, then dim(X) = r.

        If $x_1,...,x_r$ are not independent, then there is at least two
        $c_k$ $\not =$ 0 where:

        \hspace{0.5cm}
        0 = $c_1x_1 + ... + c_rx_r$

        since if only one $c_k$ $\not =$ 0, then 
        0 = $c_1x_1 + ... + c_rx_r$ = $c_kx_k$ which implies $x_k$ = 0
        since $c_k$ $\not =$ 0 which is a contradiction.
        Thus, for $c_k,c_{i_1},...,c_{i_n}$ $\not =$ 0:

        \hspace{0.5cm}
        0 = $c_1x_1 + ... + c_rx_r$
        = $c_kx_k + c_{i_1}x_{i_1} + ... + c_{i_n}x_{i_n}$
        \hspace{0.7cm}
        $\Rightarrow$
        \hspace{0.7cm}
        $x_k$ = $\frac{-c_{i_1}}{c_k}x_{i_1} + ... + \frac{-c_{i_n}}{c_k}x_{i_n}$

        Remove $x_k$ from $x_1,...,x_r$ and repeat the process
        until all $x_i$ are independent and thus,
        dim(X) = r - (\# $x_i$ removed) $<$ r.
    \end{proof}

    \newpage



    \begin{corollary}{dim(X) = (\# vectors in a basis)}{14cm}
        If $x_1,...,x_n$ is a basis for X, then dim(X) = n.
        
        Thus, dim($\mathbb{R}^n$) = n.
    \end{corollary}

    \begin{proof}
        Since $x_1,...,x_n$ is a basis for X, then
        $x_1,...,x_n$ spans X and are independent.

        Since $x_1,...,x_n$ span X, then by {\color{red} theorem 16.1.2},
        then dim(X) $\leq$ n.
        Since $x_1,...,x_n$ are independent, then dim(X) $\geq$ n
        since there might be another $x_i$ independent to $x_1,...,x_n$
        and another and so on.
        Thus, dim($\mathbb{R}^n$) = n.

        Since $e_1,...,e_n$ is a basis for $\mathbb{R}^n$, then
        dim($\mathbb{R}^n$) = n.
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Properties of Basis}{14cm}
        For vector space X where dim(X) = n:

        \begin{enumerate}[label=(\alph*), itemsep=0.1cm]
            \item n vectors span X if and only if the n vectors are independent

            \item X has a basis where every basis have only n vectors
            
            \item For independent $x_1,...,x_r$ where r $\in$ \{1,...,n\},
                X has a basis with $x_1,...,x_r$
        \end{enumerate}        
    \end{wtheorem}

    \begin{intuition}
        $x_1,...,x_m$ can span X, but not independent since there might
        be a $x_i$ that is dependent on the other $x_i$
        (aka $x_i$ = $a_ix_i + ... + a_{i-1}x_{i-1}
                    + a_{i+1}x_{i+1} + ... + a_mx_m$).
        
        $x_1,...,x_k$ can be independent, but not span X since there might
        be another x that is independent to each $x_i$
        (aka x $\not =$ $b_1x_1 + ... + b_kx_k$ for any $b_1,...,b_k$).
        
        So to get a basis, either remove the dependent elements from
        $x_1,...,x_m$ to get independent or add independent elements
        to $x_1,...,x_k$ to get a span of X.
        Simply, a basis has a set amount of vectors, but
        $x_1,...,x_m$ has too much while $x_1,...,x_k$ has too few.
    \end{intuition}

    \begin{proof}
        Let $x_1,...,x_n$ span X. If $x_1,...,x_n$ are not independent,
        then remove $x_i$ until $x_1,...,x_k$ are independent
        as performed in {\color{red} theorem 16.1.2}.
        Thus, dim(X) = k $<$ n which is a contradiction and thus,
        $x_1,...,x_n$ are independent.

        For independent $x_1,...,x_n$, add $y_1,...,y_k$ $\in$ X
        so $x_1,...,x_n,y_1,...,y_k$ span X.
        Since dim(X) = n, then $x_1,...,x_n,y_1,...,y_k$ are not independent.
        Since any non-independent set can remove elements in its span until it is
        independent and thus, preserves its span as performed in
        {\color{red} theorem 16.1.2}, then each $y_i$ can be removed to reach
        independent $x_1,...,x_n$ which still spans X.

        \rule[0.1cm]{15.3cm}{0.01cm}

        By part (a), any n independent vectors spans X so thus, forms a basis
        for X.
        For $x_1,...,x_k$ where k $>$ n, since dim(X) = n, then
        $x_1,...,x_k$ is non-independent and is thus, not a basis.
        For $x_1,...,x_k$ where k $<$ n, since dim(X) = n, there is a x $\in$ X
        such that $x_1,...,x_k,x$ are independent.
        Then x $\not =$ $c_1x_1 + ... + c_kx_k$ for any $c_1,...,c_k$ else
        
        \hspace{0.5cm}
        x = $c_1x_1 + ... + c_kx_k$
        \hspace{0.7cm}
        $\Rightarrow$
        \hspace{0.7cm}
        0 = $c_1x_1 + ... + c_kx_k + -x$

        so $x_1,...,x_k,x$ are not independent.
        Thus, there is a x $\in$ X that is not in the span of $x_1,...,x_k$
        so $x_1,...,x_k$ does not span X.

        \rule[0.1cm]{15.3cm}{0.01cm}

        For independent $x_1,...,x_r$, since dim(X) = n, there are
        $x_{r+1},...,x_n$ such that $x_1,...,x_n$ are independent.
        By part (a), $x_1,...,x_n$ spans X so $x_1,...,x_n$ forms a basis
        which contain $x_1,...,x_r$.
    \end{proof}
    
    \newpage



    \begin{definition}{Linear Transformation}{14cm}
        A mapping A of vector space X into vector space Y is
        a linear transformation if for all $x_1,x_2$ $\in$ X and scalar c:

        \hspace{0.5cm}
        A($x_1+x_2$) = A$x_1$ + A$x_2$
        \hspace{1cm}
        A($cx_1$) = $c$A$x_1$

        Since A0 + A0 = A(0+0) = A0, then A0 = 0.

        \vspace{0.2cm}

        If $x_1,...,x_n$ is a basis for X, then for any x $\in$ X, there
        is a unique set of $c_1,...,c_n$ where x = $c_1x_1 + ... + c_nx_n$
        such that:

        \hspace{0.5cm}
        Ax = A($c_1x_1 + ... + c_nx_n$)
        = $c_1$A$x_1$ + ... + $c_n$A$x_n$

        \vspace{0.2cm}

        Linear transformation that maps X into X are
        {\color{lblue} linear operators}.

        Additionally, if A is $\underline{\text{1-1}}$ and
        $\underline{\text{maps X onto X}}$,
        then A is {\color{lblue} invertible}.
        
        Thus, there is a $A^{-1}$ such that:

        \hspace{0.5cm}
        $A^{-1}$(Ax) = x
        \hspace{1cm}
        for all x $\in$ X

        Since A maps X onto X, for any x $\in$ X, then Ax = y $\in$ X.

        Thus, for all y $\in$ X, then x = $A^{-1}$(Ax) = $A^{-1}$y. Thus:

        \hspace{0.5cm}
        A($A^{-1}$y) = Ax = y

        Also, for any $x_1,x_2$ $\in$ X and scalars $c_1,c_2$
        where A$x_1$ = $y_1$ and A$x_2$ = $y_2$:

        \hspace{0.5cm}
        $A^{-1}(c_1y_1 + c_2y_2)$
        = $A^{-1}(c_1Ax_1 + c_2Ax_2)$
        = $A^{-1}(A(c_1x_1 + c_2x_2))$

        \hspace{3.6cm}
        = $c_1x_1 + c_2x_2$
        = $c_1A^{-1}(y_1) + c_2A^{-1}(y_2)$

        So, $A^{-1}$ is a linear transformation.
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Linear Operators imply 1-1 $\rightleftharpoons$ onto}{14cm}
        Linear operator A preserves independence if and only if A is 1-1.

        Thus, linear operator A is 1-1 if and only if A(X) = X.
    \end{wtheorem}

    \begin{proof}
        Let $x_1,...,x_n$ be a basis for X where each A$x_i$ = $y_i$ $\in$ X.
        So for any y $\in$ A(X), there is x $\in$ X where
        x = $c_1x_1 + ... + c_nx_n$ for a unique set of $c_1,...,c_n$
        such that:

        \hspace{0.5cm}
        y = Ax = A($c_1x_1 + ... + c_nx_n$)
        = $c_1$A$x_1$ + ... + $c_n$A$x_n$
        = $c_1y_1 + ... + c_ny_n$

        If A is 1-1, then there is only one such x so
        in respect to $y_1,...,y_n$, then any

        y = $k_1y_1 + ... + k_ny_n$
        must have $k_1 = c_1, ... , k_n = c_n$.
        Thus, for y = 0, since 0 = A0 and $x_1,...,x_n$ are independent,
        then $c_1$ = ... = $c_n$ = 0 so $y_1,...,y_n$ are independent.
        
        If A is not 1-1, then there is y where there are at least two distinct
        such x so in respect to $y_1,...,y_n$, then y = $k_1y_1 + ... + k_ny_n$
        holds true for at least 2 distinct $k_1,...,k_n$ so
        $y_1,...,y_n$ are not independent.
        Thus, A is 1-1 if and only if $y_1,...,y_n$ is independent.
        By {\color{red} theorem 16.1.4a}, $y_1,...,y_n$ span X
        so A(X) = X if and only if $y_1,...,y_n$ are independent.
    \end{proof}

    \newpage



    \begin{definition}{Operations of Linear Transformatons}{14cm}
        Let L(X,Y) be the set of all linear transformation
        of X into Y.

        Let $\Omega$ be the set of all invertible linear operators
        on $\mathbb{R}^n$.

        \begin{enumerate}[label=(\alph*), leftmargin=0.2cm, itemsep=0.1cm]
            \item If $A_1,A_2$ $\in$ L(X,Y) and $c_1,c_2$ are scalars,
                then for any x $\in$ X, define:

                \hspace{0.5cm}
                ($c_1A_1 + c_2A_2$)x = $c_1A_1x + c_2A_2x$

            \item For vector space Z, if A $\in$ L(X,Y) and B $\in$ L(Y,Z),
                then for any x $\in$ X, define:

                \hspace{0.5cm}
                (BA)x = B(Ax) $\in$ L(X,Z)

            \item For A $\in$ L($\mathbb{R}^n,\mathbb{R}^m$),
                define the norm:
                
                \hspace{0.5cm}
                $||A||$ = sup($|Ax|$ {\color{lblue} $|$}
                                x $\in$ $\mathbb{R}^n$ where $|x|$ $\leq$ 1)

            \item $|Ax|$
                = $|A(|x|\frac{x}{|x|})|$
                = $|A(\frac{x}{|x|})| \ |x|$
                $\leq$ $\text{sup}(|A(\frac{x}{|x|})|) \ |x|$
                = $||A|| \ |x|$

                If there is a $\lambda$ such that $|Ax|$ $\leq$ $\lambda|x|$
                for all x $\in$ $\mathbb{R}^n$, then
                $||A||$ $\leq$ $\lambda|1|$ = $\lambda$.

            \item For A,B $\in$ L($\mathbb{R}^n,\mathbb{R}^m$),
                the distance between A and B is defined $||A-B||$
        \end{enumerate}
    \end{definition}

    \vspace{0.5cm}



    \begin{ltheorem}{Operations of Norms of Linear Transformations}{1.5cm}
        \item If A $\in$ L($\mathbb{R}^n,\mathbb{R}^m$), then $||A||$ $<$ $\infty$.
            Thus, A is uniformly continuous.
            
            \begin{proof}[14cm]
                For $|x| \leq 1$:

                \hspace{0.5cm}
                $|Ax|$
                = $|A(x_1e_1 + ... + x_ne_n)|$
                $\leq$ $|x_1| |Ae_1| + ... + |x_n| |Ae_n|$

                \hspace{1.4cm}
                $\leq$ $|Ae_1| + ... + |Ae_n|$
                = M

                Thus,
                $||Ax||$ $\leq$ $|Ae_1| + ... + |Ae_n|$ = M $<$ $\infty$.

                Let $|x-y| < \epsilon$ and thus,
                $|Ax - Ay|$
                = $|A(x-y)|$
                $\leq$ $||A|| \ |x-y|$
                $<$ $M\epsilon$ so A is uniformly continuous.
            \end{proof}

        \item If A,B $\in$ L($\mathbb{R}^n,\mathbb{R}^m$) and c is a scalar, then:
        
            \hspace{0.5cm}
            $||A+B||$ $\leq$ $||A||$ + $||B||$
            \hspace{1cm}
            $||cA||$ = $|c| \ ||A||$

            \begin{proof}[14cm]
                For $|x| \leq 1$,
                $|(A+B)x|$
                $\leq$ $|Ax+Bx|$
                $\leq$ $|Ax| + |Bx|$
                $\leq$ $||A|| + ||B||$.

                Thus, $||A+B||$ $\leq$ $||A|| + ||B||$.
                Since $|cAx|$ = $|c| |Ax|$, then $||cA||$ = $|c| \ ||A||$.

                Also, for the distance between A and B, by part a:

                \hspace{0.5cm}
                $||A-B||$
                $\leq$ $||A+B||$
                $\leq$ $||A|| + ||B||$
                $\leq$ $M_1 + M_2$
            \end{proof}

        \item If A $\in$ L($\mathbb{R}^n,\mathbb{R}^m$) and
            B $\in$ L($\mathbb{R}^m,\mathbb{R}^k$), then:

            \hspace{0.5cm}
            $||BA||$ $\leq$ $||B|| \ ||A||$

            \begin{proof}[14cm]
                For $|x| \leq 1$, 
                $|BAx|$
                = $|B(Ax)|$
                $\leq$ $||B|| \ |Ax|$
                $\leq$ $||B|| \ ||A|| \ |x|$
                $\leq$ $||B|| \ ||A||$.

                Thus, $||BA||$ $\leq$ $||B|| \ ||A||$.
            \end{proof}
    \end{ltheorem}

    \newpage



    \begin{ltheorem}{Operations of Norms of Invertible Linear Operators}{1.5cm}
        \item If A $\in$ $\Omega$ and B $\in$ $L(\mathbb{R}^n,\mathbb{R}^n)$
            where $||B-A|| \ ||A^{-1}||$ $<$ 1, then B $\in$ $\Omega$

            \begin{proof}[14cm]
                $\frac{1}{||A^{-1}||} |x|$
                = $\frac{1}{||A^{-1}||} |A^{-1}Ax|$
                $\leq$ $\frac{1}{||A^{-1}||} ||A^{-1}|| \ |Ax|$

                \hspace{1.4cm}
                = $|Ax|$
                $\leq$ $|(A-B)x| + |Bx|$
                $\leq$ $||A-B|| \ |x| + |Bx|$

                Thus,
                $|Bx|$
                $\geq$ $(\frac{1}{||A^{-1}||} - ||A-B||) \ |x|$
                $\geq$ $\frac{2}{||A^{-1}||} |x|$ $\geq$ 0
                so Bx $\not =$ 0 if x $\not =$ 0
                so B is 1-1.
                Then by {\color{red} theorem 16.1.4a}, B spans $\mathbb{R}^n$
                so B is invertible so B $\in$ $\Omega$. 
            \end{proof}

        \item $\Omega$ $\subset$ L($\mathbb{R}^n,\mathbb{R}^n$) is open
            and the mapping T: A $\rightarrow$ $A^{-1}$ is continuous on $\Omega$

            \begin{proof}[14cm]
                Since $||B-A||$ $<$ $\frac{1}{||A^{-1}||}$ for any B $\in$ $\Omega$,
                then for every B $\in$ $\Omega$, there exist an open subset
                of L($\mathbb{R}^n,\mathbb{R}^n$) that contains B
                so $\Omega$ is open.
                
                Since

                \hspace{0.5cm}
                $|y|$ = $|BB^{-1}y|$
                $\geq$ $(\frac{1}{||A^{-1}||} - ||A-B||) \ |B^{-1}y|$
                
                \hspace{1.1cm}
                $\geq$ $(\frac{1}{||A^{-1}||} - ||A-B||) \ ||B^{-1}|| \ |y|$

                then $\frac{1}{\frac{1}{||A^{-1}||} - ||A-B||}$ $\geq$ $||B^{-1}||$.
                Thus, by {\color{red} theorem 16.1.8}:

                \hspace{0.5cm}
                $||B^{-1} - A^{-1}||$
                = $||B^{-1}(A - B)A^{-1}||$

                \hspace{3.1cm}
                $\leq$ $||B^{-1}|| \ ||A - B|| \ ||A^{-1}||$
                $\leq$ $\frac{||A - B|| \ ||A^{-1}||}
                            {\frac{1}{||A^{-1}||} - ||A-B||}$

                Since $\lim_{B \rightarrow A}$ $||A-B||$ $\rightarrow$ 0
                so $||B^{-1} - A^{-1}||$ $\rightarrow$ , then
                T is continuous on $\Omega$.
            \end{proof}
    \end{ltheorem}

    \newpage



    \begin{definition}{Matrices}{14cm}
        Let $x_1,...,x_n$ be a basis for X and $y_1,...,y_m$ be a basis for Y.

        Then every A $\in$ L(X,Y) determines a set of numbers
        $a_{ij}$ such that:

        \hspace{0.5cm}
        $Ax_j$ = $\sum_{i=1}^m a_{ij}y_i$
        \hspace{1cm}
        for j = \{1,...,n\}

        Thus, A can be represented by an m by n matrix:

        \hspace{0.5cm}
        [A] = 
        $
        \begin{bmatrix}
            a_{11} & a_{12} & ... & a_{1n} \\
            a_{21} & a_{22} & ... & a_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{m1} & a_{m2} & ... & a_{mn}
        \end{bmatrix}
        $

        Since the $a_{ij}$ of $Ax_j$ are from the j-th column [A],
        then $Ax_j$ is called the column vector of [A].
        Thus, the span(A) is the span of the column vectors of [A].

        \vspace{0.3cm}



        For any x $\in$ X, there is a unique set of $c_1,...,c_n$
        such that x = $c_1x_1 + ... + c_nx_n$:

        \vspace{0.2cm}

        \hspace{0.5cm}
        [Ax] = 
        $
        \begin{bmatrix}
            {\color{blue} (y_1) \ \{} &
                \color{orange} \overbrace{{\color{black} a_{11}}}^{c_1} &
                \color{orange} \overbrace{{\color{black} a_{12}}}^{c_2} & ... &
                \color{orange} \overbrace{{\color{black} a_{1n}}}^{c_n} & \\
            
            {\color{blue} (y_2) \ \{} &
            a_{21}  & a_{22} & ... & a_{2n}  \\

            & \vdots & \vdots & \ddots & \vdots \\

            {\color{blue} (y_m) \ \{} &
            a_{m1} & a_{m2} & ... & a_{mn}
        \end{bmatrix}
        $

        \vspace{0.2cm}

        \hspace{0.5cm}
        Ax
        = A($\sum_{j=1}^n$ $c_jx_j$)

        \hspace{1.15cm}
        = $\sum_{j=1}^n$ $c_j$A$x_j$

        \hspace{1.15cm}
        = $\sum_{j=1}^n$ $c_j$ $\sum_{i=1}^m$ $a_{ij} y_i$

        \hspace{1.15cm}
        = $\sum_{j=1}^n$ $\sum_{i=1}^m$ $a_{ij} c_j y_i$

        \hspace{1.15cm}
        = $\sum_{i=1}^m$ [$\sum_{j=1}^n a_{ij} c_j$] $y_i$

        So $[\sum_{j=1}^n a_{1j} c_j],...,[\sum_{j=1}^n a_{mj} c_j]$
        are Ax's coordinates in respect to $y_1,...,y_m$.

        \vspace{0.3cm}



        Let A $\in$ L(X,Y) and B $\in$ L(Y,Z). Then, BA $\in$ L(X,Z).

        Let $z_1,...,z_p$ be a basis for Z where:

        \hspace{0.5cm}
        B$y_i$ = $\sum_{k=1}^p$ $b_{ki} z_k$
        \hspace{1cm}
        (BA)$x_j$ = $\sum_{k=1}^p$ $c_{kj} z_k$

        Thus, B as a p by m matrix and BA as a p by n matrix can be represented:

        \hspace{0.5cm}
        [B] = 
        $
        \begin{bmatrix}
            b_{11} & b_{12} & ... & b_{1m} \\
            b_{21} & b_{22} & ... & b_{2m} \\
            \vdots & \vdots & \ddots & \vdots \\
            b_{p1} & b_{p2} & ... & b_{pm}
        \end{bmatrix}
        $
        \hspace{1cm}
        [BA] = 
        $
        \begin{bmatrix}
            c_{11} & c_{12} & ... & c_{1n} \\
            c_{21} & c_{22} & ... & c_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            c_{p1} & c_{p2} & ... & c_{pn}
        \end{bmatrix}
        $

        \vspace{0.2cm}

        \hspace{0.5cm}
        (BA)$x_j$
        = B(A$x_j$)
        = B($\sum_{i=1}^m$ $a_{ij}y_i$)

        \hspace{1.95cm}
        = $\sum_{i=1}^m$ $a_{ij}$B$y_i$

        \hspace{1.95cm}
        = $\sum_{i=1}^m$ $a_{ij}$ $\sum_{k=1}^p$ $b_{ki} z_k$

        \hspace{1.95cm}
        = $\sum_{i=1}^m$ $\sum_{k=1}^p$ $b_{ki} a_{ij} z_k$

        \hspace{1.95cm}
        = $\sum_{k=1}^p$ [$\sum_{i=1}^m$ $b_{ki} a_{ij}$] $z_k$

        Thus, $c_{kj}$ = $\sum_{i=1}^m$ $b_{ki} a_{ij}$
        for j = \{1,...,n\} and k = \{1,...,p\}.
        
        So to get matrix [BA] from [B] and [A]:

        $
        \begin{bmatrix}
            \color{red} b_{11} & \color{red} b_{12} & ... & \color{red} b_{1m} \\
            \color{blue} b_{21} & \color{blue} b_{22} & ... & \color{blue} b_{2m} \\
            \vdots & \vdots & \ddots & \vdots \\
            \color{green} b_{p1} & \color{green} b_{p2} & ... & \color{green} b_{pm}
        \end{bmatrix}
        \begin{bmatrix}
            \color{purple} a_{11} & \color{pink} a_{12} & ... & \color{orange} a_{1n} \\
            \color{purple} a_{21} & \color{pink} a_{22} & ... & \color{orange} a_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            \color{purple} a_{m1} & \color{pink} a_{m2} & ... & \color{orange} a_{mn}
        \end{bmatrix}
        $
        =
        $
        \begin{bmatrix}
            \sum_{i=1}^m {\color{red}b_{1i}} {\color{purple}a_{i1}}
            & \sum_{i=1}^m {\color{red}b_{1i}} {\color{pink}a_{i2}}
            & ...
            & \sum_{i=1}^m {\color{red}b_{1i}} {\color{orange}a_{in}} \\

            \sum_{i=1}^m {\color{blue}b_{2i}} {\color{purple}a_{i1}}
            & \sum_{i=1}^m {\color{blue}b_{2i}} {\color{pink}a_{i2}}
            & ...
            & \sum_{i=1}^m {\color{blue}b_{2i}} {\color{orange}a_{in}} \\

            \vdots & \vdots & \ddots & \vdots \\

            \sum_{i=1}^m {\color{green}b_{pi}} {\color{purple}a_{i1}}
            & \sum_{i=1}^m {\color{green}b_{pi}} {\color{pink}a_{i2}}
            & ...
            & \sum_{i=1}^m {\color{green}b_{pi}} {\color{orange}a_{in}} \\
        \end{bmatrix}
        $
    \end{definition}

    \newpage

    \begin{adjustbox}{minipage=14cm, right, vspace=0.1cm 0cm}
        For A $\in$ L($\mathbb{R}^n,\mathbb{R}^m$), since
        Ax = $\sum_{i=1}^m$ [$\sum_{j=1}^n a_{ij} c_j$] $e_i$
        where x = $\sum_{j=1}^n c_j e_j$,
        then by the Cauchy-Schwarz Inequality:

        \hspace{0.5cm}
        $|Ax|^2$
        = $\sum_{i=1}^m$ [$\sum_{j=1}^n a_{ij} c_j$]$^2$

        \hspace{1.6cm}
        $\leq$ $\sum_{i=1}^m$ [($\sum_{j=1}^n a_{ij}^2$) ($\sum_{j=1}^n c_j^2$)]

        \hspace{1.6cm}
        = [$\sum_{i=1}^m$ $\sum_{j=1}^n a_{ij}^2$] ($\sum_{j=1}^n c_j^2$)

        \hspace{1.6cm}
        = [$\sum_{i=1}^m$ $\sum_{j=1}^n a_{ij}^2$] $|x|^2$

        \vspace{0.1cm}

        Thus, for $|x|$ $\leq$ 1, then
        $||A||$
        $\leq$ $\sqrt{\sum_{i=1}^m \sum_{j=1}^n a_{ij}^2}$.
    \end{adjustbox}

    \vspace{0.5cm}



    \begin{wtheorem}
    {A linear transformation of continuous functions is continuous}{14cm}
        If each $a_{ij}$ is a continuous function on S and for each p $\in$ S,
        then $A_p$ $\in$ L($\mathbb{R}^n,\mathbb{R}^m$) with entries $a_{ij}(p)$,
        then the mapping T: S $\rightarrow$ L($\mathbb{R}^n,\mathbb{R}^m$)
        is continuous.
    \end{wtheorem}

    \begin{proof}
        Since each $a_{i,j}$ is continuous, then for $\epsilon > 0$,
        there is a $\delta > 0$ such that for t,p $\in$ S where $|t-p| < \delta$,
        then $|a_{i,j}(t) - a_{i,j}(p)| < \frac{\epsilon}{\sqrt{mn}}$.
        Thus, for $|t-p| < \delta$:

        \hspace{0.5cm}
        $||A_p - A_t||$
        $\leq$ $\sqrt{\sum_{i=1}^m \sum_{j=1}^n (a_{ij}(p) - a_{ij}(t))^2}$
        $<$ $\sqrt{\sum_{i=1}^m \sum_{j=1}^n (\frac{\epsilon}{\sqrt{mn}})^2}$
        = $\epsilon$
    \end{proof}

    \vspace{0.5cm}





\subsection{ Differentiation }

    \begin{definition}{Derivative Extended to Higher Dimensions}{14cm}
        First, let's redefine the derivative such that it can be extended to
        higher dimensions.
        For f: (a,b) $\subset$ $\mathbb{R}$ $\rightarrow$ $\mathbb{R}^m$,
        let f'(x) = y $\in$ $\mathbb{R}^m$ such that:

        \hspace{0.5cm}
        f(x+h) - f(x) = yh + r(h)
        \hspace{1cm}
        where $\lim_{h \rightarrow 0}$ $\frac{r(h)}{h}$ = 0

        Since y: h $\rightarrow$ hy is a linear transformation
        from $\mathbb{R}$ to $\mathbb{R}^m$, then
        f'(x) $\in$ L($\mathbb{R},\mathbb{R}^m$).

        \vspace{0.3cm}

        Now for derivatives in higher dimensions.
        
        Let f: x $\in$ open E $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^m$.
        
        If there is an A $\in$ L($\mathbb{R}^n,\mathbb{R}^m$) such that
        for any h $\in$ E:

        \hspace{0.5cm}
        f(x+h) - f(x) = Ah + $r_A(h)$
        \hspace{1cm}
        where $\lim_{h \rightarrow 0}$ $\frac{|r_A(h)|}{|h|}$ = 0

        then f is differentiable at x.
        Then differential of f at x, f'(x) = A.

        If f is differentiable at every x $\in$ E, then f is differentiable on E.
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{The derivative of a function is unique}{14cm}
        Let f: x $\in$ open E $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^m$.
        Let A $\in$ L($\mathbb{R}^n,\mathbb{R}^m$) such that
        for any h $\in$ E:

        \hspace{0.5cm}
        f(x+h) - f(x) = Ah + $r_A(h)$
        \hspace{1cm}
        where $\lim_{h \rightarrow 0}$ $\frac{|r_A(h)|}{|h|}$ = 0

        Suppose A = $A_1$ and A = $A_2$ satisfies such conditions.
        Then $A_1$ = $A_2$.
    \end{wtheorem}

    \begin{proof}
        For any h $\in$ $\mathbb{R}^n$:
        
        \hspace{0.5cm}
        $|(A_2-A_1)h|$
        = $|[f(x+h)-f(x)-r_{A_1}(h)] - [f(x+h)-f(x)-r_{A_2}(h)]|$

        \hspace{2.9cm}
        = $|r_{A_2}(h) - r_{A_1}(h)|$

        \hspace{2.9cm}
        $\leq$ $|r_{A_2}(h)| + |r_{A_1}(h)|$

        Since $A_1,A_2$ $\in$ L($\mathbb{R}^n,\mathbb{R}^m$),
        for any t where h is fixed, then:

        \hspace{0.5cm}
        $|(A_2-A_1)(th)|$ $\leq$ $|r_{A_2}(th)| + |r_{A_1}(th)|$

        \hspace{0.5cm}
        $|t| |(A_2-A_1)h|$ $\leq$ $|r_{A_2}(th)| + |r_{A_1}(th)|$

        \hspace{0.5cm}
        $|(A_2-A_1)h|$ $\leq$ $\frac{|r_{A_2}(th)|}{|t|}
                                + \frac{|r_{A_1}(th)|}{|t|}$

        So as t $\rightarrow$ 0, then
        $\frac{|r_{A_2}(th)|}{|t|} + \frac{|r_{A_1}(th)|}{|t|}$ $\rightarrow$
        $0+0$ = 0. Thus, $A_1$ = $A_2$.
    \end{proof}

    \newpage



    \begin{wtheorem}{Derivative of a Linear Transformation}{14cm}
        If A $\in$ L($\mathbb{R}^n,\mathbb{R}^m$) and x $\in$ $\mathbb{R}^n$, then:

        \hspace{0.5cm}
        A'(x) = A
    \end{wtheorem}

    \begin{proof}
        Since A $\in$ L($\mathbb{R}^n,\mathbb{R}^m$), then let f(x) = Ax.

        \hspace{0.5cm}
        f(x+h) - f(x)
        = A(x+h) - Ax
        = Ax + Ah - Ax
        = Ah

        Thus, $r_A(h)$  = 0
        so $\lim_{h \rightarrow 0}$ $\frac{|r_A(h)|}{|h|}$
        = $\lim_{h \rightarrow 0}$ 0 = 0. Thus, A'(x) = f'(x) = A.
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Chain Rule in Higher Dimensions}{14cm}
        Let f: open E $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^m$
        be differentiable at $x_0$ $\in$ E and
        g: f(E) $\subset$ open H $\subset$ $\mathbb{R}^m$ $\rightarrow$
        $\mathbb{R}^k$ be differentiable at f($x_0$).
        
        Then F: E $\rightarrow$ $\mathbb{R}^k$ where F(x) = g(f(x))
        is differetiable at $x_0$ such that:

        \hspace{0.5cm}
        F'($x_0$) = g'(f($x_0$)) f'($x_0$)
    \end{wtheorem}

    \begin{proof}
        Since f is differentiable at $x_0$ and g is differentiable at f($x_0$),
        then there is a A = f'($x_0$) and B = g'(f($x_0$)) such that:

        \hspace{0.5cm}
        f($x_0$+h) - f($x_0$)
        = Ah + $r_A(h)$
        \hspace{2cm}
        where $\lim_{h \rightarrow 0}$ $\frac{|r_A(h)|}{|h|}$ = 0

        \hspace{0.5cm}
        g(f($x_0$)+k) - g(f($x_0$))
        = Bk + $r_B(k)$
        \hspace{1cm}
        where $\lim_{k \rightarrow 0}$ $\frac{|r_B(k)|}{|k|}$ = 0

        Let k = f($x_0$+h) - f($x_0$). Thus:

        \hspace{0.5cm}
        F($x_0$+h) - F($x_0$) - BAh
        = g(f($x_0$+h)) - g(f($x_0$)) - BAh

        \hspace{4.8cm}
        = g(f($x_0$)+k) - g(f($x_0$)) - BAh
        = Bk + $r_B(k)$ - BAh

        \hspace{4.8cm}
        = B(k - Ah) + $r_B(k)$
        = B(f($x_0$+h) - f($x_0$) - Ah) + $r_B(k)$

        \hspace{4.8cm}
        = B$r_A(h)$ + $r_B(k)$

        \hspace{0.5cm}
        $\frac{|F(x_0+h) - F(x_0) - BAh|}{|h|}$
        = $\frac{|Br_A(h) + r_B(k)|}{|h|}$
        $\leq$ $\frac{|Br_A(h)| + |r_B(k)|}{|h|}$
        $\leq$ $\frac{||B|| \ |r_A(h)| + |r_B(k)|}{|h|}$

        Since f is differentiable at $x_0$, then f is continuous at $x_0$
        and thus, $\lim_{h \rightarrow 0}$ k = 0.

        Since $\lim_{h \rightarrow 0}$ $\frac{|r_A(h)|}{|h|}$ = 0
        and $\lim_{k \rightarrow 0}$ $\frac{|r_A(k)|}{|k|}$ = 0, then:

        \hspace{0.5cm}
        $\lim_{h \rightarrow 0}$ $\frac{|F(x_0+h) - F(x_0) - BAh|}{|h|}$
        $\leq$ $\lim_{h \rightarrow 0}$ $||B|| \frac{|r_A(h)|}{|h|}$
                + $\lim_{h \rightarrow 0}$ $\frac{|r_B(k)|}{|h|}$
        = $0+0$ = 0

        Thus, F'($x_0$) = BA = g'(f($x_0$)) f'($x_0$).
    \end{proof}

    \vspace{0.5cm}



    \begin{definition}
    {Partial Derivatives: Derivatives along the standard basis}{14cm}
        Let f: open E $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^m$.
        The components of f are the $f_1,...,f_m$ $\in$ $\mathbb{R}$ such that
        for x $\in$ E, then f(x) = $\sum_{i=1}^m$ $f_i(x) e_i$.

        Since $e_i \cdot e_j$ =
        $
        \begin{cases}
            0 & i \not = j \\
            1 & i = j
        \end{cases}$, then
        f(x) $\cdot$ $e_i$
        = $[\sum_{i=1}^m$ $f_i(x) e_i]$ $\cdot$ $e_i$
        = $f_i(x)$.

        \vspace{0.2cm}

        Then for x $\in$ E and i $\in$ \{1,...,m\} and j $\in$ \{1,...,n\},
        let the {\color{lblue} partial derivative}
        $\frac{\partial f_i}{\partial x_j}$ = $D_jf_i$
        be the derivative of $f_i$ with respect to $x_j$.
        Then for t $\in$ $\mathbb{R}$:

        \hspace{0.5cm}
        $f_i(x+te_j) - f_i(x)$ = $D_jf_i(te_j)$ + $r_{D_jf_i}(te_j)$
        \hspace{0.7cm}
        where $\lim_{t \rightarrow 0}$ $\frac{|r_{D_jf_i}(te_j)|}{|t|}$ = 0
    \end{definition}

    \newpage



    \begin{wtheorem}{Derivative of f is the sum of all partial derivatives}{14cm}
        Let f: open E $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^m$
        be differentiable at x $\in$ E. Then the partial derivatives
        ($D_jf_i$)(x) exists such that for j $\in$ \{1,...,n\}:

        \hspace{0.5cm}
        f'(x)$e_j$ = $\sum_{i=1}^m$ $(D_jf_i)(x)e_i$
    \end{wtheorem}

    \begin{proof}
        For a fixed j, since f is differentiable at x, then:

        \hspace{0.5cm}
        f(x+$te_j$) - f(x) = f'(x)($te_j$) + r($te_j$)
        \hspace{1cm}
        where $\lim_{t \rightarrow 0}$ $\frac{|r(te_j)|}{|t|}$ = 0

        Then f'(x) exist where:

        \hspace{0.5cm}
        $\lim_{t \rightarrow 0}$ $\frac{f(x+te_j) - f(x)}{t}$
        = $\lim_{t \rightarrow 0}$ $\frac{f'(x)(te_j)}{t}$ + $\frac{r(te_j)}{t}$
        = $\lim_{t \rightarrow 0}$ $t\frac{f'(x)e_j}{t}$
        = $f'(x)e_j$

        Since f(x) = $\sum_{i=1}^m f_i(x)e_i$, then:

        \hspace{0.5cm}
        $\lim_{t \rightarrow 0}$ $\frac{f(x+te_j) - f(x)}{t}$
        = $\lim_{t \rightarrow 0}$
            $\sum_{i=1}^m$ $\frac{f_i(x+te_j) - f_i(x)}{t}e_i$
        = $f'(x)e_j$

        Since f'(x) exist and
        $\lim_{t \rightarrow 0}$ $\frac{f_i(x+te_j) - f_i(x)}{t}$ = $D_jf_i(x)$,
        then each $D_jf_i(x)$ exists where:
        
        \hspace{0.5cm}
        f'(x)$e_j$
        = $\sum_{i=1}^m$ $\lim_{t \rightarrow 0}$
            $\frac{f_i(x+te_j) - f_i(x)}{t}e_i$
        = $\sum_{i=1}^m$ $(D_jf_i)(x)e_i$
    \end{proof}

    \vspace{0.5cm}



    \begin{definition}{Matrix of the Differential of f}{14cm}
        By {\color{red} theorem 16.2.6},
        f'(x)$e_j$ = $\sum_{i=1}^m$ $(D_jf_i)(x)e_i$
        where $(D_jf_i)(x)$ is the derivative of the
        component $f_i$ in respect to $x_j$ for j = \{1,...,n\}.

        Since $f'(x)e_j$ is the j-th column of [f'(x)], then:

        \vspace{0.1cm}

        \hspace{0.5cm}
        [f'(x)] = 
        $
        \begin{bmatrix}
            \sum_{i=1}^m (D_1f_i)(x)e_i
            & \sum_{i=1}^m (D_2f_i)(x)e_i
            & ...
            & \sum_{i=1}^m (D_nf_i)(x)e_i
        \end{bmatrix}
        $

        where each $\sum_{i=1}^m (D_jf_i)(x)e_i$ is a column vector
        at the j-th column.
        
        Since each $\sum_{i=1}^m (D_jf_i)(x)e_i$ has a coordinate
        of $(D_jf_i)(x)$ for $e_i$ where each
        
        $e_i$ =
        $(\underset{\scriptscriptstyle 1}{0},...,
                    \underset{\scriptscriptstyle i-1}{0},
                    \underset{\scriptscriptstyle i}{1},
                    \underset{\scriptscriptstyle i+1}{0},...,
                    \underset{\scriptscriptstyle m}{0})$
        $\in$ $\mathbb{R}^m$, then:

        \vspace{0.1cm}

        \hspace{0.5cm}
        [f'(x)] =
        $
        \begin{bmatrix}
            (D_1f_1)(x) & (D_2f_1)(x) & ... & (D_nf_1)(x) \\
            (D_1f_2)(x) & (D_2f_2)(x) & ... & (D_nf_2)(x) \\
            \vdots & \vdots & \ddots & \vdots \\
            (D_1f_m)(x) & (D_2f_m)(x) & ... & (D_nf_m)(x) \\
        \end{bmatrix}
        $

        Thus, for x $\in$ $\mathbb{R}^n$ where
        x = $x_1e_1 + ... + x_ne_n$, then:

        \hspace{0.5cm}
        f'(x)x
        = $f'(x)[\sum_{j=1}^n x_j e_j]$

        \hspace{1.7cm}
        = $\sum_{j=1}^n$ $x_j f'(x) e_j$

        \hspace{1.7cm}
        = $\sum_{j=1}^n$ $x_j \sum_{i=1}^m$ $(D_jf_i)(x)e_i$

        \hspace{1.7cm}
        = $\sum_{i=1}^m$ [$\sum_{j=1}^n$ $x_j (D_jf_i)(x)$] $e_i$
    \end{definition}

    \newpage



    \begin{definition}{Gradient and Directional Derivative}{14cm}
        Let $\gamma$: (a,b) $\subset$ $\mathbb{R}$
                    $\rightarrow$ open E $\subset$ $\mathbb{R}^n$
        and f: E $\subset$ $\mathbb{R}^n$
                $\rightarrow$ $\mathbb{R}$ both be differentiable.

        Then by {\color{red} theorem 16.2.4},
        g: $\mathbb{R}$ $\rightarrow$ $\mathbb{R}$ 
        defined as g(t) = f($\gamma(t)$)
        is differentiable for any t $\in$ (a,b) such that:
        
        \hspace{0.5cm}
        g'(t) = f'($\gamma(t)$) $\gamma'(t)$

        Since f($\gamma(t)$): E $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}$,
        by {\color{red} theorem 16.2.6}, then:
        
        \hspace{0.5cm}
        f'($\gamma(t)$)$e_j$ = $(D_jf)(\gamma(t))$
        for j = \{1,...,n\}

        Since $\gamma$: (a,b) $\subset$ $\mathbb{R}$
                    $\rightarrow$ open E $\subset$ $\mathbb{R}^n$,
        then:

        \hspace{0.5cm}
        $\gamma'(t)$
        = $\sum_{i=1}^n$ $(D_1\gamma_i)(t)e_i$
        = $\sum_{i=1}^n$ $\gamma_i'(t)e_i$

        Thus,
        g'(t)
        = $\sum_{i=1}^n$ $(D_if)(\gamma(t))$ $\gamma_i'(t)$.

        \vspace{0.5cm}

        For each x $\in$ E, let the {\color{lblue} gradient} of
        f: E $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}$ at x,
        $(\nabla f)$(x):
        
        \hspace{0.5cm}
        $(\nabla f)$(x) = $\sum_{i=1}^n$ $(D_if)(x)e_i$

        \vspace{0.5cm}

        Since $e_ie_j$ = 1 if i = j, but $e_ie_j$ = 0 if i $\not =$ j, then:

        \hspace{0.5cm}
        [f($\gamma(t)$)]' = g'(t)
        
        \hspace{2.15cm}
        = $\sum_{i=1}^n$ $(D_if)(\gamma(t))$ $\gamma_i'(t)$

        \hspace{2.15cm}
        = $\sum_{i=1}^n$ [$(D_if)(\gamma(t))e_i$ $\cdot$ $\gamma_i'(t)e_i$]

        \hspace{2.15cm}
        = [$\sum_{i=1}^n$ $(D_if)(\gamma(t))e_i$]
        $\cdot$ [$\sum_{i=1}^n$ $\gamma_i'(t)e_i$]
        = $(\nabla f)(\gamma(t))$ $\cdot$ $\gamma'(t)$

        For t $\in$ ($-\infty,\infty$), let $\gamma(t)$ = $x + tu$
        where x $\in$ E and unit vector u $\in$ $\mathbb{R}^n$. Then:

        \hspace{0.5cm}
        $(D_uf)(x)$
        = $\lim_{t \rightarrow 0}$ $\frac{f(x+tu) - f(x)}{t}$
        = $\lim_{t \rightarrow 0}$ $\frac{g(t) - g(0)}{t}$
        = g'(x)

        \hspace{2.3cm}
        = $(\nabla f)(\gamma(x))$ $\cdot$ $\gamma'(x)$
        = $(\nabla f)(x)$ $\cdot$ $u$

        Let $(D_uf)(x)$ be the {\color{lblue} directional derivative}
        of f at x in direction of u.

        For u = $u_1e_1 + ... + u_ne_i$:

        \hspace{0.5cm}
        $(D_uf)(x)$
        = $(\nabla f)(x)$ $\cdot$ $u$
        = $\sum_{i=1}^n$ $(D_if)(x)e_i$ $\cdot$ $\sum_{i=1}^n$ $u_ie_i$
        = $\sum_{i=1}^n$ $(D_if)(x)u_i$

        \vspace{0.3cm}

        Also, for a fixed f and x, $(D_uf)(x)$ is maximized when
        u = $\lambda (\nabla f)(x)$ for $\lambda > 1$
        since $x \cdot y$ = $|x| |y| \cos(\theta)$ where $\theta$ is the angle
        between x and y.
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}
    {A bounded derivative over a convex space have bounded range}{14cm}
        For differentiable f: convex open E $\subset$ $\mathbb{R}^n$
        $\rightarrow$ $\mathbb{R}^m$, there is a M $\in$ $\mathbb{R}$ such that
        $||f'(x)||$ $\leq$ M for every x $\in$ E. Then for all a,b $\in$ E:
        
        \hspace{0.5cm}
        $|f(b) - f(a)|$ $\leq$ M$|b-a|$
    \end{wtheorem}

    \begin{proof}
        For fixed a,b $\in$ E, let $\gamma(t)$ = (1-t)a + tb.
        Since E is convex, for t $\in$ [0,1], then $\gamma(t)$ $\in$ E.
        Let g(t) = f($\gamma(t)$).
        Then g'(t) = f'($\gamma(t)$)$\gamma'(t)$
        = f'($\gamma(t)$)(b-a). Thus, for t $\in$ [0,1]:

        \hspace{0.5cm}
        $|g'(t)|$
        = $|f'(\gamma(t))(b-a)|$
        $\leq$ $||f'(\gamma(t))||$ $|b-a|$
        $\leq$ M$|b-a|$

        Since g(0) = f($\gamma(0)$) = f(a)
        and g(1) = f($\gamma(1)$) = f(b), then by the Mean Value Theorem,
        for x $\in$ (0,1)

        \hspace{0.5cm}
        $|f(b) - f(a)|$
        = $|g(1) - g(0)|$
        $\leq$ $(1-0) |g'(x)|$
        $\leq$ M$|b-a|$
    \end{proof}

    \vspace{0.5cm}



    \begin{corollary}{If the derivative is 0, the function is constant}{14cm}
        For differentiable f: convex open E $\subset$ $\mathbb{R}^n$
        $\rightarrow$ $\mathbb{R}^m$, f'(x) = 0 for all x $\in$ E.
        
        Then, f is constant.
    \end{corollary}

    \begin{proof}
        Since $||f'(x)||$ = 0 for all x $\in$ E,
        then by {\color{red} theorem 7.2.9}, for all a,b $\in$ E:
        
        \hspace{0.5cm}
        0 $\leq$ $|f(b) - f(a)|$ $\leq$ $0(b-a)$ = 0

        Thus, f(b) = f(a) for all a,b $\in$ E so f is constant.
    \end{proof}

    \newpage



    \begin{definition}{Continuously Differentiable}{14cm}
        A differentiable f: open E $\subset$ $\mathbb{R}^n$
        $\rightarrow$ $\mathbb{R}^m$ is continuously differentiable
        in E if:
        
        \hspace{0.5cm}
        f': E $\rightarrow$ L($\mathbb{R}^n,\mathbb{R}^m$) is continuous

        For $\epsilon > 0$, there is a $\delta > 0$ such that for
        every x,y $\in$ E where $|x-y| < \delta$, then:

        \hspace{0.5cm}
        $||f'(y) - f'(x)|| < \epsilon$

        If f is continuous differentiable, then f $\in$ $\mathscr{C}'(E)$.
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}
    {Continuously differentiable imply continuous partial derivatives}{14cm}
        Let f: open E $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^m$.
        Then f $\in$ $\mathscr{C}'(E)$ if and only if
        each partial derivative $D_jf_i$ exist and are continuous on E.
    \end{wtheorem}

    \begin{proof}
        If f $\in$ $\mathscr{C}'(E)$, then f is differentiable.
        Thus, by {\color{red} theorem 16.2.6}, partial derivative $D_jf_i$
        where j = \{1,...,n\} exists for any x $\in$ E such that:

        \hspace{0.5cm}
        f'(x)$e_j$ = $\sum_{i=1}^m$ $(D_jf_i)(x)e_i$
        \hspace{1cm}
        $\Rightarrow$
        \hspace{1cm}
        $(D_jf_i)(x)$ = $f'(x)e_j \cdot e_i$

        Thus, since f $\in$ $\mathscr{C}'(E)$, then for $|x-y| < \delta$:

        \hspace{0.5cm}
        $|(D_jf_i)(y) - (D_jf_i)(x)|$
        = $|f'(y)e_j \cdot e_i - f'(x)e_j \cdot e_i|$
        = $|[f'(y) - f'(x)]e_j \cdot e_i|$

        \hspace{4.75cm}
        $\leq$ $|[f'(y) - f'(x)]e_j|$ $|e_i|$
        $\leq$ $||f'(y) - f'(x)||$ $|e_j|$ $|e_i|$

        \hspace{4.75cm}
        = $||f'(y) - f'(x)||$ $<$ $\epsilon$

        Thus, each $D_jf_i$ is continuous.

        \rule[0.1cm]{15.3cm}{0.01cm}

        Since each $D_jf_i$ is continuous, then for $\epsilon > 0$,
        there is a $\delta > 0$ such that for $|y-x| < \delta$, then
        for all j $\in$ \{1,...,n\} and i $\in$ \{1,...,m\}, then
        $|D_jf_i(y) - D_jf_i(x)|$ $<$ $\epsilon$.

        Then for h = $h_1e_1 + ... + h_ne_n$ where $|x-h| < \delta$:

        \hspace{0.5cm}
        $\lim_{h \rightarrow 0}$
        $\frac{|f(x+h) - f(x)
                - \sum_{i=1}^m [\sum_{j=1}^n (D_jf_i)(x)h_j]e_i|}{|h|}$

        \hspace{0.5cm}
        = $\lim_{h \rightarrow 0}$
        $\frac{|\sum_{i=1}^m [f_i(x+h_1e_1+...+h_ne_n) - f_i(x)]e_i
                - \sum_{i=1}^m [\sum_{j=1}^n (D_jf_i)(x)h_j]e_i|}{|h|}$

        \hspace{0.5cm}
        = $\lim_{h \rightarrow 0}$
        $\frac{|\sum_{i=1}^m [f_i(x+h_1e_1+...+h_ne_n) - f_i(x)
                            - \sum_{j=1}^n (D_jf_i)(x)h_j]e_i|}{|h|}$

        \hspace{0.5cm}
        = $\lim_{h \rightarrow 0}$
        $\frac{|\sum_{i=1}^m \Bigg[
        \overset{f_i(x+\sum_{k=1}^n h_ke_k) - f_i(x+\sum_{k=1}^{n-1} h_ke_k)}
        {\overset{+ f_i(x+\sum_{k=1}^{n-1} h_ke_k) - f_i(x+\sum_{k=1}^{n-2} h_ke_k)}
        {+ ... + f_i(x+h_1) - f_i(x) - \sum_{j=1}^n (D_jf_i)(x)h_j}}
        \Bigg] e_i|}{|h|}$

        Since each $D_jf_i$ exist, then by the Mean Value Theorem,
        for each j = \{1,...,n\}, there is a $t_j \in (0,h_j)$ such that:

        \hspace{0.5cm}
        $f_i(x+\sum_{k=1}^j h_ke_k) - f_i(x+\sum_{k=1}^{j-1} h_ke_k)$
        = $D_nf_i(x+\sum_{k=1}^{j-1} h_ke_k+t_je_j)h_j$

        Thus:

        \hspace{0.5cm}
        $\lim_{h \rightarrow 0}$
        $\frac{|f(x+h) - f(x)
                - \sum_{i=1}^m [\sum_{j=1}^n (D_jf_i)(x)h_j]e_i|}{|h|}$

        \hspace{0.5cm}
        = $\lim_{h \rightarrow 0}$
        $\frac{|\sum_{i=1}^m
        [\sum_{j=1}^n D_nf_i(x+\sum_{k=1}^{j-1} h_ke_k+t_je_j)h_j
        - \sum_{j=1}^n (D_jf_i)(x)h_j]e_i|}{|h|}$

        \hspace{0.5cm}
        $<$ $\lim_{h \rightarrow 0}$
            $\frac{|\sum_{i=1}^m
            [\sum_{j=1}^n [\epsilon h_j]]e_i|}{|h|}$
        $\leq$ $\lim_{h \rightarrow 0}$
            $\frac{|\sum_{i=1}^m [n \epsilon |h|]e_i|}{|h|}$
        = $\lim_{h \rightarrow 0}$ $\frac{\sqrt{m}n\epsilon|h|}{|h|}$
        = $\sqrt{m}n\epsilon$

        Thus, f(x) is differentiable where:

        \hspace{0.5cm}
        f'(x) =
        $
        \begin{bmatrix}
            (D_1f_1)(x) & (D_2f_1)(x) & \hdots & (D_nf_1)(x) \\
            (D_1f_2)(x) & (D_2f_2)(x) & \hdots & (D_nf_2)(x) \\
            \vdots & \vdots & \ddots & \vdots \\
            (D_1f_m)(x) & (D_2f_m)(x) & \hdots & (D_nf_m)(x) \\
        \end{bmatrix}
        $

        Thus, for $|y-x| < \delta$:

        \hspace{0.3cm}
        $||f'(y) - f'(x)||$
        $\leq$ $\sqrt{\sum_{i=1}^m \sum_{j=1}^n [(D_jf_i)(y) - (D_jf_i)(x)]^2}$
        $<$ $\sqrt{\sum_{i=1}^m \sum_{j=1}^n \epsilon^2}$
        = $\sqrt{mn}\epsilon$

        Thus, f $\in$ $\mathscr{C}'(E)$.
    \end{proof}

    \newpage





\subsection{ The Contraction Principle }

    \begin{definition}{Contraction}{14cm}
        For metric space X with metric d, then $\phi$: X $\rightarrow$ X
        is a {\color{lblue} contraction} if there is c $\in$ (0,1) such that
        for all x,y $\in$ X:

        \hspace{0.5cm}
        d($\phi$(x),$\phi$(y)) $\leq$ c d(x,y)
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Banach's Fixed Point Theorem}{14cm}
        If X is a complete metric space and $\phi$ is a contraction
        of X into X, then there is a unique x $\in$ X
        such that $\phi$(x) = x
    \end{wtheorem}

    \begin{proof}
        Let $\phi$(x) = x and $\phi$(y) = y. Since $\phi$ is a contraction,
        then d(x,y) = d($\phi$(x),$\phi$(y)) $\leq$ c d(x,y)
        would hold true only if d(x,y) = 0 so x = y.
        Thus, such a $\phi$(x) = x is unique.

        For a fixed $x_0$ $\in$ X, let \{$x_n$\} have
        $x_{n+1}$ = $\phi(x_n)$. Thus, for some c $\in$ (0,1):

        \hspace{0.5cm}
        d($x_{n+1}$,$x_n$)
        = d($\phi(x_n)$,$\phi(x_{n-1})$)
        $\leq$ c d($x_n,x_{n-1}$)

        \hspace{2.6cm}
        = c d($\phi(x_{n-1}),\phi(x_{n-2})$)
        = ...
        = $c^n$ d($x_1,x_0$)

        Thus, for $\epsilon > 0$, choose N such that
        $d(x_1,x_0)\frac{c^N}{(1-c)} < \epsilon$.
        Then for m $>$ n $\geq$ N:

        \hspace{0.5cm}
        d($x_{m}$,$x_n$)
        $\leq$ $\sum_{i=n}^{m-1}$ d($x_{i+1}$,$x_i$)
        $\leq$ $\sum_{i=n}^{m-1}$ $c^i$ d($x_1,x_0$)

        \hspace{2.25cm}
        $\leq$ d($x_1,x_0$) $\frac{c^n}{1-c}$
        $\leq$ d($x_1,x_0$) $\frac{c^N}{1-c}$
        $<$ $\epsilon$

        Thus, \{$x_n$\} is a Cauchy Sequence and since X is complete,
        then \{$x_n$\} converges to a x $\in$ X.
        Note a contraction is uniformly continuous so:

        \hspace{0.5cm}
        $\phi(x)$
        = $\lim_{n \rightarrow \infty} \phi(x_n)$
        = $\lim_{n \rightarrow \infty} x_{n+1}$
        = x
    \end{proof}

    \vspace{0.5cm}



    \begin{example}
        For y' = y where y(0) = 1, show y(x) = $e^x$ for x near 0.
    \end{example}

    \begin{tbox}
        Take the metric space of continuous functions, C[a,b],
        with the sup metric as defined in {\color{blue} definition 14.3.4}
        where 0 $\in$ [a,b].
        By {\color{red} theorem 14.3.5}, C[a,b] is complete.
        
        Then for each f $\in$ C[a.b], let Tf(x) = 1 + $\int_0^x$ f(t) dt
        for x $\in$ [a,b].
        
        \hspace{0.5cm}
        $|Tf(x) - Tg(x)|$
        = $|\int_0^x f(t)-g(t) dt|$
        $\leq$ $\int_{\text{min}(0,x)}^{\text{max}(0,x)} | f(t)-g(t) | dt$

        \hspace{3.55cm}
        $\leq$ $|x-0|$ d(f,g)
        $\leq$ (b-a) d(f,g)

        Thus, d(Tf(x),Tg(x)) $\leq$ (b-a) d(f,g) so for b-a $<$ 1, then T
        is a contraction.
        By {\color{red} theorem 16.3.2},
        there is a unique y where y(x) = 1 + $\int_0^x$ y(t) dt.
        To determine y, use the process defined in {\color{red} theorem 16.3.2}'s
        proof referred as the Picard iteration. Using any continuous f(x),
        let's take f(x) = 1. Then:

        \hspace{0.5cm}
        T(1)
        = 1 + $\int_0^x$ 1 dt
        = 1 + x

        \hspace{0.5cm}
        T(T(1))
        = 1 + $\int_0^x$ 1+t dt
        = 1 + x + $\frac{1}{2}x^2$

        \hspace{0.5cm}
        T(T(T(1)))
        = 1 + $\int_0^x$ 1+t+$\frac{1}{2}t^2$ dt
        = 1 + x + $\frac{1}{2}x^2$ + $\frac{1}{6}x^3$

        Thus, by {\color{blue} definition 15.2.1}, y(x)
        = $\lim_{n \rightarrow \infty}$ T$^n(1)$
        = $\sum_{k=0}^{\infty}$ $\frac{x^k}{k!}$ = $e^x$.
    \end{tbox}

    \newpage



\subsection{ Inverse Function Theorem }

    \begin{wtheorem}{Inverse Function Theorem}{14cm}
        Suppose f: E $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}^n$
        is $\mathscr{C}'(E)$ where f(a) = b and
        Df(a) is invertible for some a $\in$ E.

        Then there exists open U,V $\subset$ $\mathbb{R}^n$ such that
        f: a $\in$ U $\rightarrow$ b $\in$ V is invertible.
    \end{wtheorem}

    \begin{proof}
        Since Df(a) is invertible, then choose $\lambda$
        such that $2\lambda ||[Df(a)]^{-1}||$ = 1.
        Since f $\in$ $\mathscr{C}'(E)$, there is a $N_r(a)$ $\subset$ E
        such that for all x $\in$ $N_r(a)$,
        then $||Df(x) - Df(a)||$ $<$ $\lambda$.

        Let $\phi(x)$ = x + $[Df(a)]^{-1}$(y-f(x)) for x $\in$ E and
        y $\in$ $\mathbb{R}^n$.
        Thus, $\phi(x)$ = x if and only if y = f(x).
        Since f is differentiable, then $\phi$ is differentiable where:
        
        \hspace{0.5cm}
        $\phi'(x)$
        = $I_{n \times n}$ - $[Df(a)]^{-1}$Df(x)
        = $[Df(a)]^{-1}$(Df(a) - Df(x))

        \hspace{0.5cm}
        $||\phi'(x)||$
        = $||[Df(a)]^{-1}$(Df(a)-Df(x))$||$
        $\leq$ $||[Df(a)]^{-1}||$ $||$Df(a)-Df(x)$||$
        $<$ $\frac{1}{2\lambda} \lambda$
        = $\frac{1}{2}$

        Thus, by {\color{red} theorem 16.2.9}, then for all
        $x_1,x_2$ $\in$ $N_r(a)$:

        \hspace{0.5cm}
        $|\phi(x_2) - \phi(x_1)|$
        $\leq$ $\frac{1}{2}|x_2 - x_1|$

        so $\phi$ is a contraction and since $N_r(a)$ $\subset$ $\mathbb{R}^n$
        is complete, then by {\color{red} theorem 16.3.2},
        there is a unique x $\in$ $N_r(a)$ where
        $\phi(x)$ = x so y = f(x) for some unique x.

        Let U = $N_r(a)$ and V = f($N_r(a)$) so f is 1-1 in U and onto V
        so f is invertible on U.
        
        To show V is open, let $y_0$ $\in$ V where $y_0$ = f($x_0$) for
        some $x_0$ $\in$ U.
        
        Since f is invertible on U, then for y where $|y - y_0| < \lambda r$:

        \hspace{0.5cm}
        $|\phi(x_0) - x_0|$
        = $|[Df(x_0)]^{-1}(y-f(x_0))|$
        $\leq$ $||[Df(x_0)]^{-1}||$ $|y-y_0|$
        $<$ $\frac{1}{2\lambda} \lambda r$
        = $\frac{r}{2}$

        Thus, for x $\in$ $N_r(x_0)$:

        \hspace{0.5cm}
        $|\phi(x) - x_0|$
        $\leq$ $|\phi(x) - \phi(x_0)|$ + $|\phi(x_0) - x_0|$
        $<$ $\frac{1}{2}|x-x_0|$ + $\frac{r}{2}$
        $<$ $\frac{1}{2}r + \frac{r}{2}$
        = r

        so $\phi(x)$ $\in$ $N_r(x_0)$.
        Thus, as shown above, there is a unique x $\in$ $N_r(x_0)$
        where $\phi(x)$ = x so y = f(x) $\in$ V
        where also y $\in$ $N_{\lambda r}(y_0)$.
    \end{proof}










































